#!/usr/bin/env python3
"""
Complete task fields with only confirmed valid AirTable fields.
Final version: Uses only description, notes, priority, status.
"""

import os
import json
import time
from datetime import datetime
from pyairtable import Api

# AirTable configuration
BASE_ID = os.getenv('AIRTABLE_BASE_ID')
API_KEY = os.getenv('AIRTABLE_API_KEY')

# Load from secrets if not in environment
if not API_KEY or not BASE_ID:
    try:
        with open('/home/micha/bqx_ml_v3/.secrets/github_secrets.json', 'r') as f:
            secrets = json.load(f)
            API_KEY = API_KEY or secrets['secrets']['AIRTABLE_API_KEY']['value']
            BASE_ID = BASE_ID or secrets['secrets']['AIRTABLE_BASE_ID']['value']
    except:
        print("Warning: Could not load AirTable credentials")

# Initialize API
api = Api(API_KEY)
base = api.base(BASE_ID)
tasks_table = base.table('Tasks')

def complete_task_fields():
    """Complete valid fields to achieve 90+ scoring."""
    print("=" * 80)
    print("COMPLETING TASK FIELDS - FINAL VERSION")
    print(f"Timestamp: {datetime.now().isoformat()}")
    print("=" * 80)

    # Get all tasks
    tasks = tasks_table.all()

    # Target tasks
    target_tasks = {
        'MP03.P09.S01.T99': 'Batch Prediction',
        'MP03.P11.S02.T98': 'Interval Glossary',
        'MP03.P05.S04.T97': 'Vertex AI Datasets',
        'MP03.P09.S04.T96': 'Scheduled Retraining',
        'MP03.P09.S01.T95': 'Scheduled Predictions',
        'MP03.P08.S02.T94': 'Confusion Matrix',
        'MP03.P08.S03.T93': 'Residual Analysis'
    }

    updated_count = 0
    failed_count = 0

    for task in tasks:
        task_id = task['fields'].get('task_id', '')

        if task_id in target_tasks:
            print(f"\nüìù Updating {task_id} ({target_tasks[task_id]})...")

            # Build comprehensive description and notes
            update_fields = {}

            # Check current content
            current_desc = task['fields'].get('description', '')
            current_notes = task['fields'].get('notes', '')

            # Only update if content is minimal
            if len(current_desc) < 500 or len(current_notes) < 500:

                # Add comprehensive content based on task type
                if task_id == 'MP03.P09.S01.T99':
                    update_fields['description'] = """**Configure Vertex AI Batch Prediction Jobs for All 28 Currency Pairs**

**Objective**: Implement comprehensive batch prediction infrastructure using Vertex AI to generate BQX predictions at specific future intervals (N+45, N+90, N+180, N+360, N+720, N+1440, N+2880).

**Context**: This task completes Vertex AI process coverage (18/18) by adding scalable batch prediction capabilities. All predictions follow INTERVAL-CENTRIC architecture.

**Team**: ML Engineering Team
**Estimated Duration**: 24 hours (3-4 days elapsed)
**Dependencies**: Model deployment complete, Feature pipeline operational

**Deliverables**:
‚Ä¢ Batch prediction configuration for all 28 models
‚Ä¢ Cloud Scheduler job definitions
‚Ä¢ BigQuery output table schemas
‚Ä¢ Monitoring dashboard configuration
‚Ä¢ Cost optimization strategy
‚Ä¢ Performance benchmarks and SLAs

**Success Criteria**:
‚Ä¢ All 28 models have batch prediction configured
‚Ä¢ 99.9% job success rate achieved
‚Ä¢ Predictions available within 5-minute SLA
‚Ä¢ Cost per prediction < $0.001
‚Ä¢ Monitoring and alerting operational"""

                    update_fields['notes'] = """### Vertex AI Batch Prediction Implementation

**CRITICAL**: All predictions are at specific future INTERVALS, not time periods.
‚Ä¢ N+45 means 45 intervals ahead (NOT 45 minutes)
‚Ä¢ N+90 means 90 intervals ahead (NOT 90 minutes)
‚Ä¢ All windows use ROWS BETWEEN for interval-based calculations

**Architecture Components**:
‚Ä¢ Vertex AI Batch Prediction API for scalable inference
‚Ä¢ Cloud Scheduler for automated execution
‚Ä¢ BigQuery for input features and output predictions
‚Ä¢ Cloud Monitoring for observability

**Implementation Code**:
```python
from google.cloud import aiplatform

def create_batch_prediction(pair, horizon):
    job = aiplatform.BatchPredictionJob.create(
        job_display_name=f"bqx_batch_{pair}_h{horizon}i",
        model_name=f"projects/{project}/models/bqx_{pair}",
        input_dataset_format="bigquery",
        input_dataset_uri=f"bq://{project}.features.{pair}_interval_features",
        output_dataset_format="bigquery",
        output_dataset_uri=f"bq://{project}.predictions.{pair}_n_plus_{horizon}",
        machine_type="n1-standard-4",
        max_replica_count=10,
        generate_explanation=True
    )
```

**Scheduling Strategy**:
‚Ä¢ Short intervals (N+45, N+90): Hourly execution
‚Ä¢ Medium intervals (N+180, N+360): Every 4 hours
‚Ä¢ Long intervals (N+720, N+1440, N+2880): Daily

**BigQuery Output Schema**:
```sql
CREATE TABLE predictions.batch_{pair} (
    prediction_timestamp TIMESTAMP,
    current_interval_index INT64,  -- N
    prediction_horizon INT64,      -- 45, 90, 180, etc.
    predicted_bqx_value FLOAT64,   -- BQX at N+horizon
    confidence_lower FLOAT64,
    confidence_upper FLOAT64,
    model_version STRING
) PARTITION BY DATE(prediction_timestamp)
```

**Quality Controls**:
‚Ä¢ Input validation: Check feature completeness
‚Ä¢ Output validation: Verify prediction ranges
‚Ä¢ Performance monitoring: Track latency
‚Ä¢ Cost monitoring: Alert on anomalies

**Risk Mitigation**:
‚Ä¢ Preemptible instances for cost reduction
‚Ä¢ Retry logic for transient failures
‚Ä¢ Fallback to online prediction
‚Ä¢ Gradual rollout strategy"""

                elif task_id == 'MP03.P11.S02.T98':
                    update_fields['description'] = """**Create INTERVAL-CENTRIC Glossary and Notation Guide**

**Objective**: Document and standardize all interval-based terminology used in BQX ML V3, ensuring consistent usage across all teams and documentation.

**Team**: Documentation Team
**Estimated Duration**: 8 hours
**Dependencies**: INTERVAL-CENTRIC architecture definition

**Deliverables**:
‚Ä¢ Comprehensive glossary document
‚Ä¢ Notation reference with examples
‚Ä¢ Code templates and snippets
‚Ä¢ Training materials
‚Ä¢ Quick reference card
‚Ä¢ Wiki integration"""

                    update_fields['notes'] = """### INTERVAL-CENTRIC Glossary

**Core Notation**:
‚Ä¢ N = Current interval index
‚Ä¢ N+H = Future interval (H intervals ahead)
‚Ä¢ N-L = Past interval (L intervals back)
‚Ä¢ _Ni suffix = interval count (45i = 45 intervals)

**Standard Horizons**:
‚Ä¢ N+45 = 45 intervals ahead (short-term)
‚Ä¢ N+90 = 90 intervals ahead
‚Ä¢ N+180 = 180 intervals ahead
‚Ä¢ N+360 = 360 intervals ahead
‚Ä¢ N+720 = 720 intervals ahead
‚Ä¢ N+1440 = 1440 intervals ahead
‚Ä¢ N+2880 = 2880 intervals ahead (long-term)

**CRITICAL Rules**:
‚Ä¢ ALWAYS use ROWS BETWEEN (never RANGE BETWEEN)
‚Ä¢ ALWAYS specify "at interval N+H" (never "H minutes ahead")
‚Ä¢ ALWAYS use interval counts (never time periods)

**SQL Standards**:
```sql
-- CORRECT
ROWS BETWEEN 89 PRECEDING AND CURRENT ROW

-- WRONG
RANGE BETWEEN INTERVAL 90 MINUTE PRECEDING AND CURRENT ROW
```"""

                elif task_id == 'MP03.P05.S04.T97':
                    update_fields['description'] = """**Configure Vertex AI Datasets for TabularDataset Creation**

**Objective**: Create Vertex AI TabularDatasets from BigQuery feature tables for all 28 currency pairs with versioning and auto-refresh.

**Team**: Data Engineering Team
**Estimated Duration**: 16 hours
**Dependencies**: BigQuery feature tables, Feature engineering pipeline

**Deliverables**:
‚Ä¢ TabularDataset for each currency pair
‚Ä¢ Version control system
‚Ä¢ Auto-refresh pipeline
‚Ä¢ Data validation rules
‚Ä¢ Access controls
‚Ä¢ Documentation"""

                    update_fields['notes'] = """### Vertex AI Dataset Configuration

**Dataset Specifications**:
‚Ä¢ Source: BigQuery interval-based feature tables
‚Ä¢ Features: 280 interval-centric features per pair
‚Ä¢ Target: BQX values at future intervals
‚Ä¢ Refresh: Daily at 02:00 UTC

**Implementation**:
```python
dataset = aiplatform.TabularDataset.create(
    display_name=f"bqx_intervals_{pair}",
    bq_source=f"bq://{project}.features.{pair}_interval_features",
    labels={"architecture": "interval_centric"}
)
```

**INTERVAL-CENTRIC Features**:
‚Ä¢ All lags use interval counts (lag_45i, lag_90i)
‚Ä¢ All windows use ROWS BETWEEN
‚Ä¢ All targets are future intervals (N+45, N+90, etc.)"""

                elif task_id == 'MP03.P09.S04.T96':
                    update_fields['description'] = """**Configure Cloud Scheduler for Periodic Model Retraining**

**Objective**: Implement automated model retraining to maintain accuracy with latest market data across all 28 currency pairs.

**Team**: MLOps Team
**Estimated Duration**: 20 hours
**Dependencies**: Training pipeline, Model registry

**Deliverables**:
‚Ä¢ Cloud Scheduler jobs for all pairs
‚Ä¢ Retraining pipeline automation
‚Ä¢ Performance comparison framework
‚Ä¢ Deployment decision logic
‚Ä¢ Rollback procedures
‚Ä¢ Monitoring setup"""

                    update_fields['notes'] = """### Scheduled Retraining Implementation

**Schedule by Volume**:
‚Ä¢ High-volume pairs: Weekly
‚Ä¢ Medium-volume: Bi-weekly
‚Ä¢ Low-volume: Monthly

**Retraining Pipeline**:
1. Fetch latest interval-based features
2. Train with INTERVAL-CENTRIC architecture
3. Evaluate on future interval predictions
4. Deploy if performance improves >2%
5. Archive previous version

**Key Point**: All evaluations based on predicting BQX at specific future intervals (N+45, N+90, etc.)"""

                elif task_id == 'MP03.P09.S01.T95':
                    update_fields['description'] = """**Configure Scheduled Batch Prediction Jobs**

**Objective**: Implement Cloud Scheduler to trigger batch predictions at optimal intervals for systematic forecast generation.

**Team**: ML Engineering Team
**Estimated Duration**: 16 hours
**Dependencies**: Batch prediction setup, Model deployment

**Deliverables**:
‚Ä¢ Scheduler configurations
‚Ä¢ Market-aware scheduling
‚Ä¢ Output management
‚Ä¢ Quality validation
‚Ä¢ Alert setup
‚Ä¢ Performance dashboard"""

                    update_fields['notes'] = """### Scheduled Predictions

**Schedule by Horizon**:
‚Ä¢ N+45, N+90: Hourly
‚Ä¢ N+180, N+360: 4-hourly
‚Ä¢ N+720, N+1440, N+2880: Daily

All predictions for specific future intervals using INTERVAL-CENTRIC features."""

                elif task_id == 'MP03.P08.S02.T94':
                    update_fields['description'] = """**Implement Confusion Matrix for Directional Predictions**

**Objective**: Add confusion matrix analysis to evaluate directional accuracy of BQX predictions at future intervals.

**Team**: Model Evaluation Team
**Estimated Duration**: 12 hours
**Dependencies**: Model evaluation framework

**Deliverables**:
‚Ä¢ Confusion matrix module
‚Ä¢ Directional metrics
‚Ä¢ Visualization dashboards
‚Ä¢ Classification reports
‚Ä¢ Threshold optimization
‚Ä¢ MLOps integration"""

                    update_fields['notes'] = """### Confusion Matrix for Interval Predictions

**Classes**: UP, NEUTRAL, DOWN
**Evaluation**: Directional accuracy at each future interval (N+45, N+90, etc.)
**Metrics**: Precision, Recall, F1 per interval horizon"""

                elif task_id == 'MP03.P08.S03.T93':
                    update_fields['description'] = """**Implement Residual Analysis for Model Diagnostics**

**Objective**: Add comprehensive residual analysis for regression diagnostics of interval-based predictions.

**Team**: Model Evaluation Team
**Estimated Duration**: 16 hours
**Dependencies**: Model evaluation framework

**Deliverables**:
‚Ä¢ Residual analysis module
‚Ä¢ Diagnostic plots
‚Ä¢ Statistical tests
‚Ä¢ Pattern detection
‚Ä¢ Remediation recommendations
‚Ä¢ Automated reporting"""

                    update_fields['notes'] = """### Residual Analysis

**Diagnostics**: Analyze residuals per prediction interval
**Tests**: Normality, heteroscedasticity, autocorrelation
**Focus**: Validate predictions at each future interval (N+45, N+90, etc.)"""

                # Always ensure priority is set
                if not task['fields'].get('priority'):
                    if task_id in ['MP03.P09.S01.T99', 'MP03.P05.S04.T97', 'MP03.P09.S04.T96', 'MP03.P09.S01.T95']:
                        update_fields['priority'] = 'High'
                    else:
                        update_fields['priority'] = 'Medium'

                # Ensure status is set
                if not task['fields'].get('status'):
                    update_fields['status'] = 'Todo'

            if update_fields:
                try:
                    tasks_table.update(task['id'], update_fields)
                    print(f"  ‚úÖ Updated successfully")
                    updated_count += 1
                    time.sleep(0.2)
                except Exception as e:
                    print(f"  ‚ùå Failed: {e}")
                    failed_count += 1
            else:
                print(f"  ‚ÑπÔ∏è Already comprehensive")

    return updated_count, failed_count

def verify_results():
    """Verify task completeness."""
    print("\n" + "=" * 80)
    print("VERIFICATION")
    print("=" * 80)

    tasks = tasks_table.all()

    target_task_ids = [
        'MP03.P09.S01.T99', 'MP03.P11.S02.T98', 'MP03.P05.S04.T97',
        'MP03.P09.S04.T96', 'MP03.P09.S01.T95', 'MP03.P08.S02.T94',
        'MP03.P08.S03.T93'
    ]

    print("\nüìä Content Analysis:")
    total_score = 0
    task_count = 0

    for task in tasks:
        task_id = task['fields'].get('task_id', '')

        if task_id in target_task_ids:
            desc_len = len(task['fields'].get('description', ''))
            notes_len = len(task['fields'].get('notes', ''))
            has_priority = bool(task['fields'].get('priority'))
            has_status = bool(task['fields'].get('status'))

            # Estimate score
            score = 50  # Base
            if desc_len > 300: score += 20
            if notes_len > 300: score += 20
            if has_priority: score += 5
            if has_status: score += 5

            total_score += score
            task_count += 1

            print(f"\n{task_id}:")
            print(f"  Description: {desc_len} chars")
            print(f"  Notes: {notes_len} chars")
            print(f"  Priority: {'‚úÖ' if has_priority else '‚ùå'}")
            print(f"  Status: {'‚úÖ' if has_status else '‚ùå'}")
            print(f"  Estimated score: {score}")

    avg_score = total_score / task_count if task_count > 0 else 0
    print(f"\nüìä Average estimated score: {avg_score:.1f}")

    return avg_score

def main():
    """Main execution."""
    print("=" * 80)
    print("TASK FIELD COMPLETION - FINAL VERSION")
    print("=" * 80)

    # Update tasks
    updated, failed = complete_task_fields()

    # Verify
    avg_score = verify_results()

    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)

    print(f"\nüìä Results:")
    print(f"  Tasks updated: {updated}")
    print(f"  Failed: {failed}")
    print(f"  Average score: {avg_score:.1f}")

    if avg_score >= 90:
        print(f"\n‚úÖ SUCCESS! Tasks should achieve 90+ scoring")
        print(f"   All records have comprehensive INTERVAL-CENTRIC content")
    elif avg_score >= 80:
        print(f"\n‚úÖ GOOD! Tasks well-documented for 80+ scoring")
    else:
        print(f"\n‚ö†Ô∏è May need additional content for 90+ scoring")

    print(f"\nüèÅ Completed at: {datetime.now().isoformat()}")

    return 0 if avg_score >= 90 else 1

if __name__ == "__main__":
    exit(main())