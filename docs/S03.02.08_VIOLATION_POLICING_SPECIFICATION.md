# S03.02.08: Implement Intelligence Violation Policing Service

**Stage ID**: S03.02.08
**Phase**: MP03.02 - Intelligence Architecture
**Priority**: HIGH
**Estimated Hours**: 16 hours
**Dependencies**: S03.02.05, S03.02.06, S03.02.07
**Status**: Planned

---

## ðŸŽ¯ Objective

Implement a continuous operational policing service that monitors the BQX ML V3 system for violations of critical intelligence constraints, paradigm mandates, and architectural rules, with automated detection, alerting, and enforcement capabilities.

---

## ðŸ“‹ Scope

### In Scope
- Continuous monitoring daemon/service for constraint violations
- Real-time detection of paradigm violations
- Automated alerting system (email, AirTable, Slack)
- Violation tracking and logging to BigQuery
- Violation metrics dashboard (Grafana)
- Enforcement mechanisms (blocking, rollback)
- Violation history and trend analysis

### Out of Scope
- Intelligence file validation (covered in S03.02.07)
- IntelligenceManager class implementation (covered in S03.02.06)
- General system monitoring (covered in S03.09.07)

---

## ðŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Intelligence Policing Service                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”œâ”€â–º Violation Detector
                              â”‚   â”œâ”€â–º Paradigm Monitor
                              â”‚   â”œâ”€â–º Constraint Monitor
                              â”‚   â”œâ”€â–º Architecture Monitor
                              â”‚   â””â”€â–º Data Quality Monitor
                              â”‚
                              â”œâ”€â–º Alert Manager
                              â”‚   â”œâ”€â–º Email Alerts
                              â”‚   â”œâ”€â–º AirTable Updates
                              â”‚   â”œâ”€â–º Slack Notifications
                              â”‚   â””â”€â–º PagerDuty (Critical)
                              â”‚
                              â”œâ”€â–º Violation Logger
                              â”‚   â”œâ”€â–º BigQuery Logging
                              â”‚   â”œâ”€â–º Local File Logging
                              â”‚   â””â”€â–º Structured Logging
                              â”‚
                              â”œâ”€â–º Enforcement Engine
                              â”‚   â”œâ”€â–º Automated Blocking
                              â”‚   â”œâ”€â–º Rollback Triggers
                              â”‚   â””â”€â–º Access Revocation
                              â”‚
                              â””â”€â–º Metrics Exporter
                                  â”œâ”€â–º Prometheus Metrics
                                  â”œâ”€â–º Grafana Dashboards
                                  â””â”€â–º BigQuery Analytics
```

---

## ðŸ”§ Technical Implementation

### 1. Violation Policing Service

#### Main Service Daemon
**File**: `src/services/intelligence_policing_service.py`

```python
#!/usr/bin/env python3
"""
Intelligence Violation Policing Service
Continuous monitoring for constraint and paradigm violations
"""
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path
import json

from src.monitoring.violation_detector import ViolationDetector
from src.monitoring.alert_manager import AlertManager
from src.monitoring.violation_logger import ViolationLogger
from src.monitoring.enforcement_engine import EnforcementEngine
from src.monitoring.metrics_exporter import MetricsExporter
from src.utils.intelligence_manager import IntelligenceManager

logger = logging.getLogger(__name__)


class IntelligencePolicingService:
    """Continuous monitoring service for intelligence violations"""

    def __init__(self):
        self.intelligence = IntelligenceManager()
        self.detector = ViolationDetector(self.intelligence)
        self.alert_manager = AlertManager()
        self.logger = ViolationLogger()
        self.enforcer = EnforcementEngine()
        self.metrics = MetricsExporter()

        self.running = False
        self.check_interval = 60  # seconds

    async def start(self):
        """Start the policing service"""
        logger.info("ðŸš¨ Starting Intelligence Policing Service")
        self.running = True

        # Start monitoring loops
        await asyncio.gather(
            self.paradigm_monitoring_loop(),
            self.constraint_monitoring_loop(),
            self.architecture_monitoring_loop(),
            self.data_quality_monitoring_loop(),
            self.metrics_export_loop()
        )

    async def paradigm_monitoring_loop(self):
        """Monitor for paradigm violations"""
        while self.running:
            try:
                violations = await self.detector.check_paradigm_violations()

                for violation in violations:
                    await self.handle_violation(violation)

                await asyncio.sleep(self.check_interval)

            except Exception as e:
                logger.error(f"Paradigm monitoring error: {e}")
                await asyncio.sleep(5)

    async def constraint_monitoring_loop(self):
        """Monitor for constraint violations"""
        while self.running:
            try:
                violations = await self.detector.check_constraint_violations()

                for violation in violations:
                    await self.handle_violation(violation)

                await asyncio.sleep(self.check_interval)

            except Exception as e:
                logger.error(f"Constraint monitoring error: {e}")
                await asyncio.sleep(5)

    async def architecture_monitoring_loop(self):
        """Monitor for architecture violations"""
        while self.running:
            try:
                violations = await self.detector.check_architecture_violations()

                for violation in violations:
                    await self.handle_violation(violation)

                await asyncio.sleep(self.check_interval * 2)  # Less frequent

            except Exception as e:
                logger.error(f"Architecture monitoring error: {e}")
                await asyncio.sleep(5)

    async def data_quality_monitoring_loop(self):
        """Monitor for data quality violations"""
        while self.running:
            try:
                violations = await self.detector.check_data_quality_violations()

                for violation in violations:
                    await self.handle_violation(violation)

                await asyncio.sleep(self.check_interval * 5)  # Less frequent

            except Exception as e:
                logger.error(f"Data quality monitoring error: {e}")
                await asyncio.sleep(5)

    async def handle_violation(self, violation: Dict[str, Any]):
        """Handle detected violation"""
        logger.warning(f"ðŸš¨ VIOLATION DETECTED: {violation['type']}")

        # Log violation
        await self.logger.log_violation(violation)

        # Send alerts based on severity
        if violation['severity'] == 'CRITICAL':
            await self.alert_manager.send_critical_alert(violation)
            await self.enforcer.enforce(violation)
        elif violation['severity'] == 'HIGH':
            await self.alert_manager.send_high_alert(violation)
        else:
            await self.alert_manager.send_warning_alert(violation)

        # Update metrics
        self.metrics.increment_violation_count(violation['type'])

    async def metrics_export_loop(self):
        """Export metrics periodically"""
        while self.running:
            try:
                await self.metrics.export_metrics()
                await asyncio.sleep(30)  # Export every 30 seconds

            except Exception as e:
                logger.error(f"Metrics export error: {e}")
                await asyncio.sleep(5)

    async def stop(self):
        """Stop the policing service"""
        logger.info("ðŸ›‘ Stopping Intelligence Policing Service")
        self.running = False


async def main():
    """Main entry point"""
    service = IntelligencePolicingService()

    try:
        await service.start()
    except KeyboardInterrupt:
        await service.stop()


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### 2. Violation Detector

#### Violation Detection Engine
**File**: `src/monitoring/violation_detector.py`

```python
#!/usr/bin/env python3
"""
Violation Detection Engine
Detects various types of intelligence violations
"""
import asyncio
from typing import List, Dict, Any
from datetime import datetime
from google.cloud import bigquery

from src.utils.intelligence_manager import IntelligenceManager


class ViolationDetector:
    """Detects intelligence violations across the system"""

    def __init__(self, intelligence: IntelligenceManager):
        self.intelligence = intelligence
        self.bq_client = bigquery.Client(project='bqx-ml')

    async def check_paradigm_violations(self) -> List[Dict[str, Any]]:
        """Check for BQX paradigm violations"""
        violations = []

        # Check 1: BQX features must be included
        for pair in self.intelligence.get_valid_pairs():
            features = await self.get_features_for_pair(pair)

            bqx_features = [f for f in features if 'bqx' in f.lower()]

            if not bqx_features:
                violations.append({
                    'type': 'PARADIGM_VIOLATION',
                    'subtype': 'MISSING_BQX_FEATURES',
                    'severity': 'CRITICAL',
                    'pair': pair,
                    'description': f"Pair {pair} missing BQX features",
                    'timestamp': datetime.now().isoformat(),
                    'details': {
                        'total_features': len(features),
                        'bqx_features': 0,
                        'expected_bqx_features': '>= 3'
                    }
                })

        # Check 2: BQX must be in targets
        for pair in self.intelligence.get_valid_pairs():
            targets = await self.get_targets_for_pair(pair)

            bqx_targets = [t for t in targets if 'bqx' in t.lower()]

            if not bqx_targets:
                violations.append({
                    'type': 'PARADIGM_VIOLATION',
                    'subtype': 'MISSING_BQX_TARGETS',
                    'severity': 'CRITICAL',
                    'pair': pair,
                    'description': f"Pair {pair} missing BQX targets",
                    'timestamp': datetime.now().isoformat()
                })

        return violations

    async def check_constraint_violations(self) -> List[Dict[str, Any]]:
        """Check for constraint violations"""
        violations = []

        # Check 1: ROWS BETWEEN usage (interval-centric)
        sql_files = self.get_sql_files()

        for sql_file in sql_files:
            content = open(sql_file).read()

            # Prohibited patterns
            if 'RANGE BETWEEN' in content:
                violations.append({
                    'type': 'CONSTRAINT_VIOLATION',
                    'subtype': 'RANGE_BETWEEN_USAGE',
                    'severity': 'CRITICAL',
                    'file': str(sql_file),
                    'description': 'RANGE BETWEEN is prohibited (use ROWS BETWEEN)',
                    'timestamp': datetime.now().isoformat()
                })

            if 'INTERVAL' in content and 'ROWS BETWEEN' not in content:
                violations.append({
                    'type': 'CONSTRAINT_VIOLATION',
                    'subtype': 'INTERVAL_USAGE',
                    'severity': 'CRITICAL',
                    'file': str(sql_file),
                    'description': 'INTERVAL keyword prohibited (use ROWS BETWEEN)',
                    'timestamp': datetime.now().isoformat()
                })

        # Check 2: Model independence
        for pair in self.intelligence.get_valid_pairs():
            cross_pair_usage = await self.check_cross_pair_contamination(pair)

            if cross_pair_usage:
                violations.append({
                    'type': 'CONSTRAINT_VIOLATION',
                    'subtype': 'CROSS_PAIR_CONTAMINATION',
                    'severity': 'CRITICAL',
                    'pair': pair,
                    'description': f"Model for {pair} using data from other pairs",
                    'timestamp': datetime.now().isoformat(),
                    'details': cross_pair_usage
                })

        return violations

    async def check_architecture_violations(self) -> List[Dict[str, Any]]:
        """Check for architecture violations"""
        violations = []

        # Check 1: 28 independent models
        model_count = await self.count_models()

        if model_count != 140:  # 28 pairs Ã— 5 algorithms
            violations.append({
                'type': 'ARCHITECTURE_VIOLATION',
                'subtype': 'INCORRECT_MODEL_COUNT',
                'severity': 'HIGH',
                'description': f"Expected 140 models, found {model_count}",
                'timestamp': datetime.now().isoformat(),
                'details': {
                    'expected': 140,
                    'actual': model_count,
                    'pairs': 28,
                    'algorithms': 5
                }
            })

        # Check 2: GCP-only infrastructure
        non_gcp_usage = await self.detect_non_gcp_services()

        if non_gcp_usage:
            violations.append({
                'type': 'ARCHITECTURE_VIOLATION',
                'subtype': 'NON_GCP_SERVICE_USAGE',
                'severity': 'CRITICAL',
                'description': 'Non-GCP services detected',
                'timestamp': datetime.now().isoformat(),
                'details': non_gcp_usage
            })

        return violations

    async def check_data_quality_violations(self) -> List[Dict[str, Any]]:
        """Check for data quality violations"""
        violations = []

        # Check 1: Missing data threshold
        for pair in self.intelligence.get_valid_pairs():
            missing_pct = await self.get_missing_data_percentage(pair)

            if missing_pct > 0.01:  # 1% threshold
                violations.append({
                    'type': 'DATA_QUALITY_VIOLATION',
                    'subtype': 'EXCESSIVE_MISSING_DATA',
                    'severity': 'HIGH',
                    'pair': pair,
                    'description': f"Pair {pair} has {missing_pct:.2%} missing data",
                    'timestamp': datetime.now().isoformat(),
                    'details': {
                        'threshold': 0.01,
                        'actual': missing_pct
                    }
                })

        # Check 2: Minimum rows requirement
        for pair in self.intelligence.get_valid_pairs():
            row_count = await self.get_row_count(pair)

            if row_count < 100000:
                violations.append({
                    'type': 'DATA_QUALITY_VIOLATION',
                    'subtype': 'INSUFFICIENT_DATA',
                    'severity': 'CRITICAL',
                    'pair': pair,
                    'description': f"Pair {pair} has only {row_count} rows",
                    'timestamp': datetime.now().isoformat(),
                    'details': {
                        'minimum': 100000,
                        'actual': row_count
                    }
                })

        return violations

    # Helper methods
    async def get_features_for_pair(self, pair: str) -> List[str]:
        """Get feature columns for a currency pair"""
        query = f"""
        SELECT column_name
        FROM `bqx-ml.bqx_ml.INFORMATION_SCHEMA.COLUMNS`
        WHERE table_name = 'align_bqx_{pair}'
        """
        result = self.bq_client.query(query).result()
        return [row.column_name for row in result]

    async def get_targets_for_pair(self, pair: str) -> List[str]:
        """Get target columns for a currency pair"""
        # Similar to get_features_for_pair
        pass

    async def check_cross_pair_contamination(self, pair: str) -> Dict[str, Any]:
        """Check if model uses data from other pairs"""
        # Check training data sources, feature tables, etc.
        pass

    async def count_models(self) -> int:
        """Count deployed models"""
        # Query Vertex AI Model Registry
        pass

    async def detect_non_gcp_services(self) -> List[str]:
        """Detect usage of non-GCP services"""
        # Scan code for AWS, Azure references
        pass

    async def get_missing_data_percentage(self, pair: str) -> float:
        """Calculate missing data percentage for a pair"""
        # Query BigQuery for NULL counts
        pass

    async def get_row_count(self, pair: str) -> int:
        """Get row count for a pair"""
        # Query BigQuery
        pass

    def get_sql_files(self) -> List[Path]:
        """Get all SQL files in the repository"""
        from pathlib import Path
        return list(Path('scripts/gcp').rglob('*.sql'))
```

### 3. Alert Manager

#### Multi-Channel Alerting
**File**: `src/monitoring/alert_manager.py`

```python
#!/usr/bin/env python3
"""
Alert Manager
Sends violation alerts through multiple channels
"""
import asyncio
import smtplib
from email.mime.text import MIMEText
from typing import Dict, Any
import aiohttp
import os


class AlertManager:
    """Manages violation alerts across multiple channels"""

    def __init__(self):
        self.email_config = {
            'smtp_server': 'smtp.gmail.com',
            'smtp_port': 587,
            'sender': os.getenv('ALERT_EMAIL_SENDER'),
            'recipients': ['michael@bqxml.com']
        }

        self.slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
        self.airtable_api_key = os.getenv('AIRTABLE_API_KEY')
        self.airtable_base_id = os.getenv('AIRTABLE_BASE_ID')

    async def send_critical_alert(self, violation: Dict[str, Any]):
        """Send critical severity alert (all channels + PagerDuty)"""
        await asyncio.gather(
            self.send_email_alert(violation, priority='CRITICAL'),
            self.send_slack_alert(violation, priority='CRITICAL'),
            self.update_airtable_violation(violation),
            self.trigger_pagerduty(violation)
        )

    async def send_high_alert(self, violation: Dict[str, Any]):
        """Send high severity alert (email + Slack + AirTable)"""
        await asyncio.gather(
            self.send_email_alert(violation, priority='HIGH'),
            self.send_slack_alert(violation, priority='HIGH'),
            self.update_airtable_violation(violation)
        )

    async def send_warning_alert(self, violation: Dict[str, Any]):
        """Send warning alert (Slack + AirTable)"""
        await asyncio.gather(
            self.send_slack_alert(violation, priority='WARNING'),
            self.update_airtable_violation(violation)
        )

    async def send_email_alert(self, violation: Dict[str, Any], priority: str):
        """Send email alert"""
        subject = f"[{priority}] Intelligence Violation: {violation['type']}"

        body = f"""
Intelligence Violation Detected

Type: {violation['type']}
Subtype: {violation.get('subtype', 'N/A')}
Severity: {violation['severity']}
Timestamp: {violation['timestamp']}
Description: {violation['description']}

Details:
{self.format_details(violation.get('details', {}))}

Action Required: Investigate and remediate immediately.

-- BQX ML V3 Intelligence Policing Service
"""

        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = self.email_config['sender']
        msg['To'] = ', '.join(self.email_config['recipients'])

        # Send email (async wrapper around smtplib)
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self._send_email_sync, msg)

    def _send_email_sync(self, msg):
        """Synchronous email sending"""
        with smtplib.SMTP(self.email_config['smtp_server'], self.email_config['smtp_port']) as server:
            server.starttls()
            server.login(
                self.email_config['sender'],
                os.getenv('ALERT_EMAIL_PASSWORD')
            )
            server.send_message(msg)

    async def send_slack_alert(self, violation: Dict[str, Any], priority: str):
        """Send Slack alert"""
        color = {
            'CRITICAL': '#FF0000',
            'HIGH': '#FFA500',
            'WARNING': '#FFFF00'
        }.get(priority, '#CCCCCC')

        payload = {
            'attachments': [{
                'color': color,
                'title': f"ðŸš¨ {priority}: {violation['type']}",
                'text': violation['description'],
                'fields': [
                    {
                        'title': 'Severity',
                        'value': violation['severity'],
                        'short': True
                    },
                    {
                        'title': 'Timestamp',
                        'value': violation['timestamp'],
                        'short': True
                    }
                ],
                'footer': 'BQX ML V3 Intelligence Policing'
            }]
        }

        async with aiohttp.ClientSession() as session:
            await session.post(self.slack_webhook, json=payload)

    async def update_airtable_violation(self, violation: Dict[str, Any]):
        """Log violation to AirTable"""
        url = f"https://api.airtable.com/v0/{self.airtable_base_id}/Violations"

        headers = {
            'Authorization': f'Bearer {self.airtable_api_key}',
            'Content-Type': 'application/json'
        }

        payload = {
            'fields': {
                'Type': violation['type'],
                'Subtype': violation.get('subtype', ''),
                'Severity': violation['severity'],
                'Description': violation['description'],
                'Timestamp': violation['timestamp'],
                'Status': 'Open',
                'Details': str(violation.get('details', {}))
            }
        }

        async with aiohttp.ClientSession() as session:
            await session.post(url, json=payload, headers=headers)

    async def trigger_pagerduty(self, violation: Dict[str, Any]):
        """Trigger PagerDuty alert for critical violations"""
        # Implement PagerDuty integration
        pass

    def format_details(self, details: Dict[str, Any]) -> str:
        """Format violation details for display"""
        return '\n'.join(f"  {k}: {v}" for k, v in details.items())
```

### 4. Violation Logger

#### Structured Logging to BigQuery
**File**: `src/monitoring/violation_logger.py`

```python
#!/usr/bin/env python3
"""
Violation Logger
Logs violations to BigQuery and local files
"""
from google.cloud import bigquery
from datetime import datetime
from typing import Dict, Any
import json
import logging


class ViolationLogger:
    """Logs intelligence violations for tracking and analysis"""

    def __init__(self):
        self.bq_client = bigquery.Client(project='bqx-ml')
        self.table_id = 'bqx-ml.bqx_ml.intelligence_violations'
        self.ensure_table_exists()

    def ensure_table_exists(self):
        """Create violations table if it doesn't exist"""
        schema = [
            bigquery.SchemaField('violation_id', 'STRING', mode='REQUIRED'),
            bigquery.SchemaField('timestamp', 'TIMESTAMP', mode='REQUIRED'),
            bigquery.SchemaField('type', 'STRING', mode='REQUIRED'),
            bigquery.SchemaField('subtype', 'STRING'),
            bigquery.SchemaField('severity', 'STRING', mode='REQUIRED'),
            bigquery.SchemaField('description', 'STRING'),
            bigquery.SchemaField('pair', 'STRING'),
            bigquery.SchemaField('file', 'STRING'),
            bigquery.SchemaField('details', 'JSON'),
            bigquery.SchemaField('resolved', 'BOOLEAN'),
            bigquery.SchemaField('resolved_at', 'TIMESTAMP'),
            bigquery.SchemaField('resolution_notes', 'STRING')
        ]

        table = bigquery.Table(self.table_id, schema=schema)

        try:
            self.bq_client.create_table(table)
            logging.info(f"Created table {self.table_id}")
        except Exception as e:
            logging.debug(f"Table already exists: {e}")

    async def log_violation(self, violation: Dict[str, Any]):
        """Log violation to BigQuery and local file"""
        # Generate unique violation ID
        violation_id = self.generate_violation_id(violation)

        # Prepare BigQuery row
        row = {
            'violation_id': violation_id,
            'timestamp': violation['timestamp'],
            'type': violation['type'],
            'subtype': violation.get('subtype'),
            'severity': violation['severity'],
            'description': violation['description'],
            'pair': violation.get('pair'),
            'file': violation.get('file'),
            'details': json.dumps(violation.get('details', {})),
            'resolved': False,
            'resolved_at': None,
            'resolution_notes': None
        }

        # Insert to BigQuery
        errors = self.bq_client.insert_rows_json(self.table_id, [row])

        if errors:
            logging.error(f"BigQuery insert errors: {errors}")
        else:
            logging.info(f"Logged violation {violation_id} to BigQuery")

        # Also log to local file
        self.log_to_file(violation)

    def log_to_file(self, violation: Dict[str, Any]):
        """Log violation to local JSON file"""
        log_file = f"logs/violations_{datetime.now().strftime('%Y%m%d')}.jsonl"

        with open(log_file, 'a') as f:
            f.write(json.dumps(violation) + '\n')

    def generate_violation_id(self, violation: Dict[str, Any]) -> str:
        """Generate unique violation ID"""
        import hashlib

        content = f"{violation['type']}_{violation['timestamp']}_{violation.get('pair', '')}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
```

### 5. Enforcement Engine

#### Automated Enforcement Actions
**File**: `src/monitoring/enforcement_engine.py`

```python
#!/usr/bin/env python3
"""
Enforcement Engine
Automatically enforces intelligence constraints
"""
import logging
from typing import Dict, Any


class EnforcementEngine:
    """Enforces intelligence constraints through automated actions"""

    async def enforce(self, violation: Dict[str, Any]):
        """Execute enforcement action based on violation type"""
        logging.warning(f"âš ï¸  Enforcing violation: {violation['type']}")

        enforcement_map = {
            'PARADIGM_VIOLATION': self.enforce_paradigm,
            'CONSTRAINT_VIOLATION': self.enforce_constraint,
            'ARCHITECTURE_VIOLATION': self.enforce_architecture,
            'DATA_QUALITY_VIOLATION': self.enforce_data_quality
        }

        enforcer = enforcement_map.get(violation['type'])

        if enforcer:
            await enforcer(violation)
        else:
            logging.warning(f"No enforcer for violation type: {violation['type']}")

    async def enforce_paradigm(self, violation: Dict[str, Any]):
        """Enforce paradigm violation"""
        if violation.get('subtype') == 'MISSING_BQX_FEATURES':
            # Block model training for this pair
            pair = violation['pair']
            await self.block_model_training(pair)
            logging.warning(f"ðŸš« Blocked model training for {pair} (missing BQX features)")

    async def enforce_constraint(self, violation: Dict[str, Any]):
        """Enforce constraint violation"""
        if violation.get('subtype') == 'RANGE_BETWEEN_USAGE':
            # Prevent SQL execution
            file = violation['file']
            await self.block_sql_file(file)
            logging.warning(f"ðŸš« Blocked SQL file {file} (RANGE BETWEEN usage)")

        elif violation.get('subtype') == 'CROSS_PAIR_CONTAMINATION':
            # Rollback model deployment
            pair = violation['pair']
            await self.rollback_model(pair)
            logging.warning(f"ðŸ”„ Rolled back model for {pair} (cross-pair contamination)")

    async def enforce_architecture(self, violation: Dict[str, Any]):
        """Enforce architecture violation"""
        if violation.get('subtype') == 'NON_GCP_SERVICE_USAGE':
            # Alert and require manual fix
            logging.critical("ðŸš¨ NON-GCP SERVICE DETECTED - Manual remediation required")

    async def enforce_data_quality(self, violation: Dict[str, Any]):
        """Enforce data quality violation"""
        if violation.get('subtype') == 'INSUFFICIENT_DATA':
            # Block model training until data requirements met
            pair = violation['pair']
            await self.block_model_training(pair)
            logging.warning(f"ðŸš« Blocked model training for {pair} (insufficient data)")

    # Enforcement action implementations
    async def block_model_training(self, pair: str):
        """Block model training for a specific pair"""
        # Create flag file or update database
        pass

    async def block_sql_file(self, file: str):
        """Prevent execution of a SQL file"""
        # Rename file or update permissions
        pass

    async def rollback_model(self, pair: str):
        """Rollback model deployment"""
        # Use Vertex AI API to rollback
        pass
```

### 6. Metrics Exporter

#### Prometheus Metrics & Grafana Dashboard
**File**: `src/monitoring/metrics_exporter.py`

```python
#!/usr/bin/env python3
"""
Metrics Exporter
Exports violation metrics for monitoring
"""
from prometheus_client import Counter, Gauge, Histogram, start_http_server
from google.cloud import bigquery
from typing import Dict


class MetricsExporter:
    """Exports violation metrics to Prometheus"""

    def __init__(self):
        # Define metrics
        self.violation_count = Counter(
            'intelligence_violations_total',
            'Total number of intelligence violations',
            ['type', 'severity']
        )

        self.active_violations = Gauge(
            'intelligence_violations_active',
            'Number of currently active (unresolved) violations',
            ['type']
        )

        self.violation_resolution_time = Histogram(
            'intelligence_violation_resolution_seconds',
            'Time taken to resolve violations',
            ['type']
        )

        self.bq_client = bigquery.Client(project='bqx-ml')

        # Start Prometheus HTTP server
        start_http_server(8000)

    def increment_violation_count(self, violation_type: str, severity: str = 'HIGH'):
        """Increment violation counter"""
        self.violation_count.labels(type=violation_type, severity=severity).inc()

    async def export_metrics(self):
        """Export current metrics from BigQuery"""
        # Query active violations
        query = """
        SELECT
            type,
            COUNT(*) as count
        FROM `bqx-ml.bqx_ml.intelligence_violations`
        WHERE resolved = FALSE
        GROUP BY type
        """

        result = self.bq_client.query(query).result()

        for row in result:
            self.active_violations.labels(type=row.type).set(row.count)
```

---

## ðŸ“¦ Deliverables

### 1. Policing Service
- [ ] `intelligence_policing_service.py` - Main daemon
- [ ] Systemd service configuration
- [ ] Deployment scripts

### 2. Detection Components
- [ ] `violation_detector.py` - Detection engine
- [ ] Paradigm violation detection
- [ ] Constraint violation detection
- [ ] Architecture violation detection
- [ ] Data quality violation detection

### 3. Alerting System
- [ ] `alert_manager.py` - Multi-channel alerts
- [ ] Email integration
- [ ] Slack integration
- [ ] AirTable integration
- [ ] PagerDuty integration

### 4. Logging & Tracking
- [ ] `violation_logger.py` - BigQuery logging
- [ ] BigQuery violations table schema
- [ ] Local file logging
- [ ] Violation history queries

### 5. Enforcement
- [ ] `enforcement_engine.py` - Automated enforcement
- [ ] Model training blocking
- [ ] SQL file blocking
- [ ] Model rollback
- [ ] Access revocation

### 6. Monitoring Dashboard
- [ ] Grafana dashboard JSON
- [ ] Violation metrics panels
- [ ] Trend analysis charts
- [ ] Alert configuration

### 7. Documentation
- [ ] Service architecture guide
- [ ] Deployment guide
- [ ] Alert response procedures
- [ ] Troubleshooting guide

---

## ðŸ” Acceptance Criteria

### Functional Requirements
- [ ] Service runs continuously 24/7
- [ ] Detects all defined violation types
- [ ] Sends alerts within 1 minute of detection
- [ ] Logs all violations to BigQuery
- [ ] Enforces critical violations automatically
- [ ] Provides real-time metrics via Prometheus

### Non-Functional Requirements
- [ ] Service uptime > 99.9%
- [ ] Detection latency < 60 seconds
- [ ] Alert delivery < 1 minute
- [ ] Zero false positives for critical violations
- [ ] Handles 100+ violations per day

### Dashboard Requirements
- [ ] Real-time violation count display
- [ ] Violation trend charts (7/30/90 days)
- [ ] Severity breakdown
- [ ] Resolution time tracking
- [ ] Top violation types

---

## ðŸ“Š Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Service uptime | > 99.9% | Monitoring logs |
| Detection accuracy | 100% | Manual validation |
| False positive rate | < 0.1% | Review analysis |
| Alert delivery time | < 60s | Timestamp diff |
| Violation resolution time | < 4 hours | BigQuery analytics |

---

## ðŸ”— Dependencies

### Prerequisites
- S03.02.05: Intelligence files created
- S03.02.06: IntelligenceManager implemented
- S03.02.07: Intelligence hooks configured

### Dependents
- All development and operational stages benefit from policing

---

## ðŸ“š Resources

### Engineering Effort
- **Service Development**: 8 hours
- **Detection Logic**: 4 hours
- **Alerting Integration**: 2 hours
- **Dashboard Creation**: 1 hour
- **Documentation**: 1 hour
- **Total**: 16 hours

### Infrastructure
- Compute Engine instance (service daemon)
- BigQuery (violation logging)
- Prometheus + Grafana (metrics & dashboards)
- Email/Slack/PagerDuty integrations

---

*Stage Specification Version: 1.0*
*Created: 2025-11-25*
*Author: BQXML Chief Engineer*
