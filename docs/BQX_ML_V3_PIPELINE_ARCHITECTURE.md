# BQX ML V3 Pipeline Architecture
## How Feature Data Trains Models to Predict Future BQX Values

---

## üìä COMPLETE PIPELINE FLOW DIAGRAM

```mermaid
graph TB
    %% Data Source
    RAW[("üì• Raw OHLCV Data<br/>1-minute bars<br/>28 Currency Pairs")]

    %% IDX Feature Branch
    RAW --> IDX_CALC["üî∑ IDX Calculation<br/>idx_mid = (high + low) / 2<br/>idx_close = close<br/>idx_spread = ask - bid"]

    IDX_CALC --> IDX_BASE["üìä idx_features_${pair}<br/>Base IDX features"]

    IDX_BASE --> IDX_LAG["‚èÆÔ∏è lag_idx_${pair}<br/>LAG(idx, 1-180)"]
    IDX_BASE --> IDX_REGIME["üéØ regime_idx_${pair}<br/>Market regimes"]
    IDX_BASE --> IDX_AGG["üìà agg_idx_${pair}<br/>Multi-resolution"]

    %% BQX Feature Branch
    RAW --> BQX_CALC["üî∂ BQX Calculation<br/>bqx = idx_mid - AVG(future_idx)<br/>Windows: [45,90,180,360,720,1440,2880]"]

    BQX_CALC --> BQX_BASE["üìä bqx_features_${pair}<br/>Base BQX features"]

    BQX_BASE --> BQX_LAG["‚èÆÔ∏è lag_bqx_${pair}<br/>LAG(bqx, 1-180)"]
    BQX_BASE --> BQX_REGIME["üéØ regime_bqx_${pair}<br/>Momentum regimes"]
    BQX_BASE --> BQX_AGG["üìà agg_bqx_${pair}<br/>Multi-resolution"]
    BQX_BASE --> BQX_DERIV["üìâ bqx_derivatives_${pair}<br/>Velocity, Acceleration"]

    %% Feature Matrix Assembly
    IDX_LAG --> MATRIX["üîÑ Feature Matrix Assembly<br/>8 types √ó 6 categories √ó 2 variants<br/>INTERVAL-CENTRIC"]
    IDX_REGIME --> MATRIX
    IDX_AGG --> MATRIX
    BQX_LAG --> MATRIX
    BQX_REGIME --> MATRIX
    BQX_AGG --> MATRIX
    BQX_DERIV --> MATRIX

    %% Target Creation
    BQX_BASE --> TARGET["üéØ Target Creation<br/>y = LEAD(bqx_mid, h)<br/>h ‚àà [45,90,180,360,720,1440,2880]"]

    %% Model Training
    MATRIX --> TRAIN["ü§ñ Model Training<br/>28 Independent Models<br/>One per currency pair"]
    TARGET --> TRAIN

    %% Model Types
    TRAIN --> LINEAR["üìê Linear Models<br/>Ridge, Lasso"]
    TRAIN --> TREE["üå≥ Tree Models<br/>XGBoost, LightGBM"]
    TRAIN --> DEEP["üß† Deep Learning<br/>LSTM, Transformer"]

    %% Ensemble
    LINEAR --> ENSEMBLE["üîÆ Ensemble<br/>Weighted Average<br/>Meta-Learning"]
    TREE --> ENSEMBLE
    DEEP --> ENSEMBLE

    %% Predictions
    ENSEMBLE --> PREDICT["‚ú® Predictions<br/>Future BQX Values<br/>Multiple Horizons"]

    style RAW fill:#e1f5fe
    style IDX_CALC fill:#bbdefb
    style BQX_CALC fill:#ffcc80
    style MATRIX fill:#c8e6c9
    style TRAIN fill:#ce93d8
    style ENSEMBLE fill:#ff8a65
    style PREDICT fill:#81c784
```

---

## üîÑ DETAILED PIPELINE STAGES

### **Stage 1: Raw Data Ingestion**
```python
# 1-minute OHLCV bars for 28 currency pairs
raw_data = {
    'bar_start_time': timestamp,  # Aligned to minute boundaries
    'open': float,                # Opening price
    'high': float,                # Highest price in interval
    'low': float,                 # Lowest price in interval
    'close': float,               # Closing price
    'volume': float,              # Trading volume
    'bid': float,                 # Best bid price
    'ask': float                  # Best ask price
}
```

---

### **Stage 2: Dual Feature Creation (IDX & BQX)**

#### **2.1 IDX Features (Raw Indexed Values)**
```sql
-- Create base IDX features
CREATE TABLE idx_features_${pair} AS
SELECT
    bar_start_time,
    (high + low) / 2 AS idx_mid,
    close AS idx_close,
    high AS idx_high,
    low AS idx_low,
    (close - open) AS idx_change,
    (ask - bid) AS idx_spread,
    volume AS idx_volume
FROM raw_ohlcv_${pair}
```

#### **2.2 BQX Features (Backward-Looking Momentum)**
```sql
-- Create BQX features using INTERVAL-CENTRIC windows
CREATE TABLE bqx_features_${pair} AS
SELECT
    bar_start_time,

    -- BQX for each window (ROWS BETWEEN for intervals)
    idx_mid - AVG(idx_mid) OVER (
        ORDER BY bar_start_time
        ROWS BETWEEN 1 FOLLOWING AND 45 FOLLOWING
    ) AS bqx_45w,

    idx_mid - AVG(idx_mid) OVER (
        ORDER BY bar_start_time
        ROWS BETWEEN 1 FOLLOWING AND 90 FOLLOWING
    ) AS bqx_90w,

    -- ... continue for all 7 windows

FROM idx_features_${pair}
```

---

### **Stage 3: Feature Engineering Pipeline**

#### **3.1 Lag Features (Historical Values)**
```python
# Both IDX and BQX get lag features
LAG_INTERVALS = [1, 2, 3, 5, 10, 15, 30, 45, 60, 90, 120, 180]

for lag in LAG_INTERVALS:
    # IDX lags
    features[f'idx_mid_lag_{lag}i'] = LAG(idx_mid, lag)
    features[f'idx_close_lag_{lag}i'] = LAG(idx_close, lag)

    # BQX lags (critical for autoregressive modeling)
    features[f'bqx_45w_lag_{lag}i'] = LAG(bqx_45w, lag)
    features[f'bqx_90w_lag_{lag}i'] = LAG(bqx_90w, lag)
```

#### **3.2 Multi-Resolution Aggregations**
```python
RESOLUTIONS = [5, 15, 45, 90, 180, 360]

for res in RESOLUTIONS:
    # IDX aggregations
    features[f'idx_mean_{res}i'] = AVG(idx_mid) over ROWS(res)
    features[f'idx_std_{res}i'] = STDDEV(idx_mid) over ROWS(res)

    # BQX aggregations
    features[f'bqx_mean_{res}i'] = AVG(bqx_mid) over ROWS(res)
    features[f'bqx_momentum_{res}i'] = SUM(bqx_mid) over ROWS(res)
```

#### **3.3 BQX Momentum Derivatives**
```python
# Unique to BQX - captures momentum changes
features['bqx_velocity_1i'] = bqx_mid - LAG(bqx_mid, 1)
features['bqx_velocity_5i'] = (bqx_mid - LAG(bqx_mid, 5)) / 5
features['bqx_acceleration_1i'] = bqx_velocity_1i - LAG(bqx_velocity_1i, 1)
features['bqx_jerk_1i'] = bqx_acceleration_1i - LAG(bqx_acceleration_1i, 1)
```

---

### **Stage 4: Feature Matrix Assembly**

```python
# Final feature matrix structure per currency pair
feature_matrix = {
    # IDX Features (Raw market state)
    'idx_base': [...],        # 8 base features
    'idx_lags': [...],        # 12 lags √ó 8 features = 96 features
    'idx_aggregations': [...], # 6 resolutions √ó 4 stats = 24 features
    'idx_regimes': [...],     # 3 regime indicators

    # BQX Features (Momentum signals)
    'bqx_base': [...],        # 7 window features
    'bqx_lags': [...],        # 12 lags √ó 7 windows = 84 features
    'bqx_aggregations': [...], # 6 resolutions √ó 4 stats = 24 features
    'bqx_derivatives': [...],  # 4 derivative features
    'bqx_regimes': [...],     # 3 momentum regimes

    # Cross-Features
    'idx_bqx_correlation': [...], # Correlation between IDX and BQX
    'divergence_signals': [...]   # When IDX and BQX disagree
}

# Total: ~280 features per sample
```

---

### **Stage 5: Target Variable Creation**

```python
# CRITICAL: Targets are FUTURE BQX values
PREDICTION_HORIZONS = [45, 90, 180, 360, 720, 1440, 2880]

for horizon in PREDICTION_HORIZONS:
    # Create target for each horizon
    target[f'bqx_future_{horizon}i'] = LEAD(bqx_mid, horizon)

    # Multi-horizon targets for ensemble learning
    target[f'bqx_avg_future_{horizon}i'] = AVG(bqx_mid) OVER (
        ROWS BETWEEN horizon FOLLOWING AND (horizon + 45) FOLLOWING
    )
```

---

### **Stage 6: Model Training Architecture**

```python
class BQXPredictor:
    def __init__(self, pair, horizon):
        self.pair = pair
        self.horizon = horizon
        self.models = {
            'linear': Ridge(alpha=0.1),
            'xgboost': XGBRegressor(n_estimators=100),
            'lstm': LSTMModel(sequence_length=180)
        }

    def train(self, X_train, y_train):
        """Train all model types"""
        # Features include both IDX and BQX
        for name, model in self.models.items():
            if name == 'lstm':
                # Sequence features for time series
                X_seq = create_sequences(X_train, length=180)
                model.fit(X_seq, y_train)
            else:
                # Tabular features
                model.fit(X_train, y_train)

    def predict(self, X_test):
        """Ensemble prediction"""
        predictions = []
        for name, model in self.models.items():
            if name == 'lstm':
                X_seq = create_sequences(X_test, length=180)
                pred = model.predict(X_seq)
            else:
                pred = model.predict(X_test)
            predictions.append(pred)

        # Weighted ensemble
        weights = [0.2, 0.5, 0.3]  # Linear, XGBoost, LSTM
        return np.average(predictions, weights=weights, axis=0)
```

---

### **Stage 7: Training Process Flow**

```python
# For each currency pair, train independent model
for pair in CURRENCY_PAIRS:
    # Load dual feature tables
    idx_features = load_table(f'idx_features_{pair}')
    bqx_features = load_table(f'bqx_features_{pair}')

    # Combine features
    X = combine_features(idx_features, bqx_features)

    # Create targets (future BQX values)
    y = create_targets(bqx_features, horizon=90)

    # Temporal split (no data leakage!)
    X_train, X_val, X_test = temporal_split(X, [0.7, 0.15, 0.15])
    y_train, y_val, y_test = temporal_split(y, [0.7, 0.15, 0.15])

    # Train model
    model = BQXPredictor(pair, horizon=90)
    model.train(X_train, y_train)

    # Validate
    val_predictions = model.predict(X_val)
    val_score = calculate_r2(y_val, val_predictions)

    # Save model
    save_model(model, f'models/{pair}_h90_model.pkl')
```

---

## üéØ KEY INSIGHTS: WHY THIS PIPELINE WORKS

### **1. Dual Feature Advantage**
- **IDX features** capture absolute price levels and market structure
- **BQX features** capture momentum and mean-reversion tendencies
- Together they provide complementary signals for prediction

### **2. BQX as Both Feature AND Target**
- **Autoregressive power**: Past BQX values predict future BQX
- **Self-similarity**: BQX patterns tend to repeat at different scales
- **Momentum persistence**: BQX trends often continue

### **3. INTERVAL-CENTRIC Consistency**
- All calculations use `ROWS BETWEEN` (never time-based)
- Features remain consistent across weekends and market closures
- No distortion from irregular time gaps

### **4. Multi-Horizon Learning**
- Models learn patterns at multiple time scales
- Short-term noise filtered by longer horizons
- Ensemble combines predictions across horizons

---

## üìà PREDICTION FLOW EXAMPLE

```python
# Real-time prediction for EURUSD
def predict_next_bqx(pair='EURUSD', current_time=NOW):
    # 1. Get latest IDX values
    idx_current = get_latest_idx(pair, current_time)

    # 2. Get latest BQX values (already calculated)
    bqx_current = get_latest_bqx(pair, current_time)

    # 3. Compute lag features (last 180 intervals)
    idx_lags = compute_lags(idx_history, [1, 2, 3, ..., 180])
    bqx_lags = compute_lags(bqx_history, [1, 2, 3, ..., 180])

    # 4. Compute aggregations
    idx_aggs = compute_aggregations(idx_history, [5, 15, 45, 90, 180])
    bqx_aggs = compute_aggregations(bqx_history, [5, 15, 45, 90, 180])

    # 5. Compute BQX derivatives
    bqx_derivatives = compute_derivatives(bqx_history)

    # 6. Assemble feature vector
    X = np.concatenate([
        idx_current, idx_lags, idx_aggs,
        bqx_current, bqx_lags, bqx_aggs, bqx_derivatives
    ])

    # 7. Load trained model
    model = load_model(f'models/{pair}_h90_model.pkl')

    # 8. Predict future BQX (90 intervals ahead)
    future_bqx_90 = model.predict(X.reshape(1, -1))

    return future_bqx_90
```

---

## üöÄ PRODUCTION DEPLOYMENT

```yaml
Pipeline Components:
  1. Data Ingestion:
     - BigQuery streaming inserts (1-min bars)
     - Real-time IDX calculation
     - Real-time BQX calculation (using historical windows)

  2. Feature Engineering:
     - Dataflow pipeline for continuous feature updates
     - Sliding window computations (INTERVAL-CENTRIC)
     - Feature store for low-latency serving

  3. Model Serving:
     - Vertex AI endpoints (28 models)
     - Batch predictions every minute
     - Online predictions on-demand

  4. Monitoring:
     - Prediction accuracy tracking
     - Feature drift detection
     - Model performance dashboards
```

---

## üìä EXPECTED PERFORMANCE

Based on the dual feature architecture and INTERVAL-CENTRIC approach:

| Metric | Target | Rationale |
|--------|--------|-----------|
| **R¬≤ Score** | 0.35-0.55 | BQX autocorrelation + IDX state signals |
| **Directional Accuracy** | 65-70% | Momentum persistence in BQX |
| **Sharpe Ratio** | 1.5-2.2 | Risk-adjusted returns from predictions |
| **Model Stability (PSI)** | < 0.22 | INTERVAL-CENTRIC consistency |

---

## üîë SUCCESS FACTORS

1. **Feature Richness**: ~280 engineered features capture multiple market aspects
2. **Temporal Integrity**: No data leakage through strict LEAD/LAG separation
3. **Model Independence**: 28 separate models avoid cross-pair contamination
4. **Ensemble Robustness**: Multiple algorithms reduce overfitting risk
5. **Interval Consistency**: ROWS BETWEEN ensures stable features across gaps

---

*This pipeline architecture leverages both IDX and BQX features to predict future BQX values with high accuracy, implementing the paradigm shift where BQX serves as both feature and target.*