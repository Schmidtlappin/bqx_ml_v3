{
  "data_ingestion_workflow": {
    "name": "Real-Time Data Ingestion",
    "trigger": "1-minute bar completion",
    "steps": [
      {
        "step": 1,
        "name": "Fetch Market Data",
        "action": "Query market data provider API",
        "output": "Raw OHLCV data",
        "error_handling": "Retry 3 times with exponential backoff"
      },
      {
        "step": 2,
        "name": "Validate Data",
        "action": "Check schema and value ranges",
        "output": "Validated data or error",
        "error_handling": "Log error and alert if validation fails"
      },
      {
        "step": 3,
        "name": "Transform Data",
        "action": "Convert to BigQuery schema",
        "output": "Transformed data records",
        "error_handling": "Skip malformed records, log warnings"
      },
      {
        "step": 4,
        "name": "Load to BigQuery",
        "action": "Insert into regression_bqx_{pair} tables",
        "output": "28 table inserts",
        "error_handling": "Rollback on failure, retry once"
      },
      {
        "step": 5,
        "name": "Trigger Feature Pipeline",
        "action": "Pub/Sub message to feature pipeline",
        "output": "Pipeline triggered",
        "error_handling": "Queue message for retry if pipeline unavailable"
      }
    ],
    "sla": "< 5 seconds from bar close to BigQuery",
    "monitoring": ["ingestion_latency", "error_rate", "data_completeness"]
  },
  "feature_engineering_workflow": {
    "name": "Feature Table Generation",
    "trigger": "New data in regression tables",
    "steps": [
      {
        "step": 1,
        "name": "Lag Features",
        "action": "Create lag_bqx_{pair} tables with 1-60 period lags",
        "sql": "CREATE OR REPLACE TABLE lag_bqx_{pair} AS SELECT *, LAG(close, 1)...",
        "duration": "~30 seconds per pair"
      },
      {
        "step": 2,
        "name": "Regime Classification",
        "action": "Create regime_bqx_{pair} with market state features",
        "sql": "CREATE OR REPLACE TABLE regime_bqx_{pair} AS SELECT *, CASE WHEN...",
        "duration": "~45 seconds per pair"
      },
      {
        "step": 3,
        "name": "Aggregations",
        "action": "Create agg_bqx_{pair} with cross-window aggregations",
        "sql": "CREATE OR REPLACE TABLE agg_bqx_{pair} AS SELECT *, AVG(close) OVER...",
        "duration": "~60 seconds per pair"
      },
      {
        "step": 4,
        "name": "Feature Alignment",
        "action": "Create align_bqx_{pair} with final feature matrix",
        "sql": "CREATE OR REPLACE TABLE align_bqx_{pair} AS SELECT * FROM agg...",
        "duration": "~15 seconds per pair"
      },
      {
        "step": 5,
        "name": "Quality Check",
        "action": "Validate feature completeness and ranges",
        "checks": ["no_nulls", "value_ranges", "row_counts"],
        "duration": "~10 seconds per pair"
      }
    ],
    "parallelization": "All 28 pairs processed simultaneously",
    "total_duration": "~3 minutes for all pairs",
    "monitoring": ["pipeline_duration", "table_sizes", "validation_errors"]
  },
  "model_training_workflow": {
    "name": "Weekly Model Retraining",
    "trigger": "Every Sunday 00:00 UTC",
    "steps": [
      {
        "step": 1,
        "name": "Extract Training Data",
        "action": "Query last 90 days from align_bqx_{pair} tables",
        "output": "Training dataset per pair"
      },
      {
        "step": 2,
        "name": "Data Split",
        "action": "Split into train/val/test (80/10/10)",
        "output": "3 datasets per pair"
      },
      {
        "step": 3,
        "name": "Hyperparameter Tuning",
        "action": "Bayesian optimization for each algorithm",
        "iterations": 50,
        "duration": "~1 hour per pair per algorithm"
      },
      {
        "step": 4,
        "name": "Model Training",
        "action": "Train 5 algorithms per pair (140 models total)",
        "algorithms": ["RandomForest", "XGBoost", "LightGBM", "LSTM", "GRU"],
        "duration": "~30 minutes per model"
      },
      {
        "step": 5,
        "name": "Model Evaluation",
        "action": "Test set evaluation with multiple metrics",
        "metrics": ["MAE", "RMSE", "R²", "Sharpe Ratio"],
        "output": "Performance report per model"
      },
      {
        "step": 6,
        "name": "Model Registration",
        "action": "Upload to Vertex AI Model Registry",
        "metadata": ["version", "metrics", "training_date"],
        "output": "Model URI"
      },
      {
        "step": 7,
        "name": "A/B Testing",
        "action": "Deploy new model to 10% traffic for 24 hours",
        "comparison": "New model vs current production",
        "decision": "Promote if 5% better performance"
      }
    ],
    "total_duration": "~24 hours for full training cycle",
    "parallelization": "4 pairs trained simultaneously",
    "monitoring": ["training_progress", "model_performance", "resource_usage"]
  },
  "prediction_workflow": {
    "name": "Real-Time Prediction Generation",
    "trigger": "REST API request or WebSocket connection",
    "steps": [
      {
        "step": 1,
        "name": "Authenticate Request",
        "action": "Validate API key and rate limits",
        "duration": "< 10ms"
      },
      {
        "step": 2,
        "name": "Fetch Latest Features",
        "action": "Query most recent row from align_bqx_{pair}",
        "duration": "< 20ms"
      },
      {
        "step": 3,
        "name": "Load Models",
        "action": "Retrieve 5 models from cache or registry",
        "duration": "< 30ms (cached) or < 200ms (cold start)"
      },
      {
        "step": 4,
        "name": "Generate Predictions",
        "action": "Run inference with 5 algorithms",
        "output": "5 predictions per pair",
        "duration": "< 50ms"
      },
      {
        "step": 5,
        "name": "Ensemble Prediction",
        "action": "Weighted average based on recent performance",
        "output": "Final BQX prediction",
        "duration": "< 5ms"
      },
      {
        "step": 6,
        "name": "Log Prediction",
        "action": "Store prediction in BigQuery for validation",
        "async": true,
        "duration": "< 10ms async"
      },
      {
        "step": 7,
        "name": "Return Response",
        "action": "Format and send JSON response",
        "format": "{\"pair\": \"EURUSD\", \"bqx_mid\": 1.234, \"confidence\": 0.89}",
        "duration": "< 5ms"
      }
    ],
    "total_latency": "< 100ms (warm) or < 300ms (cold start)",
    "throughput": "> 1000 predictions/second",
    "monitoring": ["latency_p50", "latency_p95", "latency_p99", "error_rate"]
  },
  "deployment_workflow": {
    "name": "Blue-Green Deployment",
    "trigger": "Manual or automated after A/B test success",
    "steps": [
      {
        "step": 1,
        "name": "Build Docker Image",
        "action": "Build container with new models",
        "registry": "Google Container Registry",
        "tags": ["latest", "v{version}"]
      },
      {
        "step": 2,
        "name": "Deploy Green Environment",
        "action": "Create new Cloud Run revision",
        "traffic": "0% initially",
        "health_check": "HTTP /health endpoint"
      },
      {
        "step": 3,
        "name": "Smoke Tests",
        "action": "Run automated smoke tests against green",
        "tests": ["health", "authentication", "sample_predictions"],
        "duration": "~5 minutes"
      },
      {
        "step": 4,
        "name": "Gradual Traffic Shift",
        "action": "Shift traffic 10% → 50% → 100% over 30 minutes",
        "monitoring": "Error rate and latency",
        "rollback_trigger": "Error rate > 1% or latency > 500ms"
      },
      {
        "step": 5,
        "name": "Validation",
        "action": "Monitor green environment for 1 hour",
        "metrics": ["error_rate", "latency", "prediction_accuracy"],
        "threshold": "No degradation vs blue"
      },
      {
        "step": 6,
        "name": "Decommission Blue",
        "action": "Delete old Cloud Run revision after 24 hours",
        "retention": "Keep blue available for quick rollback"
      }
    ],
    "rollback_time": "< 2 minutes to previous revision",
    "monitoring": ["deployment_success_rate", "rollback_count", "downtime"]
  },
  "phase_25b_correlation_workflow": {
    "name": "Phase 2.5B Target-Based Feature Correlation Analysis",
    "purpose": "Correlate features against prediction targets to inform feature selection for Phase 3 model training",
    "methodology": "Features → Targets (LEAD-based)",
    "trigger": "Manual execution after feature tables complete",
    "steps": [
      {
        "step": 1,
        "name": "Generate Target Tables",
        "action": "Create targets_{pair} tables for all 28 pairs",
        "sql_pattern": "LEAD(bqx_X, H) OVER (ORDER BY interval_time)",
        "output": "28 target tables with 49 targets each (7 BQX × 7 horizons)",
        "targets": {
          "bqx_components": ["bqx_45", "bqx_90", "bqx_180", "bqx_360", "bqx_720", "bqx_1440", "bqx_2880"],
          "prediction_horizons": [15, 30, 45, 60, 75, 90, 105],
          "note": "Horizons are INTERVAL counts (row-based), NOT time-based"
        }
      },
      {
        "step": 2,
        "name": "Enumerate Feature Tables",
        "action": "Identify all feature tables across 8 types × 6 centrics × 2 variants",
        "feature_types": ["reg_", "agg_", "lag_", "regime_", "align_", "corr_", "mom_", "vol_"],
        "centrics": ["PRIMARY", "VARIANT", "COVARIANT", "TRIANGULATION", "SECONDARY", "TERTIARY"],
        "variants": ["IDX (price-based)", "BQX (momentum-based)"],
        "expected_tables": "1,870+"
      },
      {
        "step": 3,
        "name": "Calculate Feature-Target Correlations",
        "action": "CORR(feature, target) for each feature-target pair",
        "parallelization": "Batch by currency pair (5 batches)",
        "batches": [
          "Batch 1: EURUSD, GBPUSD, USDJPY, AUDUSD, USDCAD, USDCHF, NZDUSD",
          "Batch 2: EURGBP, EURJPY, EURAUD, EURCAD, EURCHF, EURNZD",
          "Batch 3: GBPJPY, GBPAUD, GBPCAD, GBPCHF, GBPNZD",
          "Batch 4: AUDJPY, AUDCAD, AUDCHF, AUDNZD",
          "Batch 5: CADJPY, CADCHF, CHFJPY, NZDJPY, NZDCAD, NZDCHF"
        ],
        "estimated_correlations": "285,000+"
      },
      {
        "step": 4,
        "name": "Store Correlation Results",
        "action": "Create feature_correlations_{pair} tables",
        "schema": ["feature_name", "feature_table", "centric", "variant", "target", "correlation", "abs_correlation", "rank"],
        "output_dataset": "bqx_ml_v3_analytics"
      },
      {
        "step": 5,
        "name": "Generate Summary Report",
        "action": "Create feature_correlation_summary table",
        "aggregations": [
          "Top 100 features by average |r| across all targets",
          "Top 20 features per target",
          "Distribution by centric type",
          "Distribution by variant (IDX vs BQX)",
          "Correlation decay by horizon"
        ]
      },
      {
        "step": 6,
        "name": "Validation",
        "checks": [
          "Row counts match source tables",
          "NULL boundary handling (last H rows NULL)",
          "No data leakage (Features=CURRENT/PAST, Targets=FUTURE)",
          "Correlation values in [-1, 1] range",
          "At least one correlation > 0.9 per pair (sanity check)"
        ]
      }
    ],
    "data_leakage_prevention": {
      "features": "Use LAG operations or current values (calculable at prediction time)",
      "targets": "Use LEAD operations exclusively (future values)",
      "validation": "Verify no feature column contains future data"
    },
    "remediation": {
      "corr_bqx_tables": {
        "description": "Create 224 corr_bqx_ibkr_{pair}_{asset} tables",
        "formula": "28 pairs × 8 external assets",
        "external_assets": ["EWA", "EWG", "EWJ", "EWU", "GLD", "SPY", "UUP", "VIX"],
        "priority": "P2 (parallel with correlation analysis)"
      },
      "skipped_by_design": [
        "COVARIANT corr_ - correlation of correlations is redundant",
        "SECONDARY corr_ - CSI feature correlations redundant",
        "TRIANGULATION lag/regime/corr - architectural design decision"
      ]
    },
    "outputs": [
      "targets_{pair} - 28 tables",
      "feature_correlations_{pair} - 28 tables",
      "feature_correlation_summary - 1 table",
      "corr_bqx_ibkr_{pair}_{asset} - 224 tables (remediation)"
    ],
    "monitoring": ["batch_completion", "correlation_distribution", "validation_errors"]
  },
  "cloud_run_backup_workflow": {
    "name": "Automated BigQuery to Box.com Disaster Recovery",
    "purpose": "Export BigQuery tables to Box.com for off-site disaster recovery independent of GCP",
    "trigger": "Cloud Scheduler (automated) or manual HTTP POST",
    "service": {
      "name": "bq-to-box-sync",
      "url": "https://bq-to-box-sync-499681702492.us-central1.run.app",
      "region": "us-central1",
      "image": "gcr.io/bqx-ml/bq-to-box-sync:latest",
      "memory": "2Gi",
      "timeout": "3600s",
      "service_account": "bq-to-box-sync@bqx-ml.iam.gserviceaccount.com"
    },
    "endpoints": {
      "health": "GET / -> {status: healthy}",
      "sync": "POST /sync -> {dataset, tables} -> sync results",
      "sync_dataset": "POST /sync/<dataset> -> sync specific dataset"
    },
    "steps": [
      {
        "step": 1,
        "name": "Receive Trigger",
        "action": "Cloud Scheduler sends OIDC-authenticated POST request",
        "authentication": "OIDC token from service account"
      },
      {
        "step": 2,
        "name": "Initialize Box Client",
        "action": "Load JWT config from Secret Manager (oxo-box-jwt-config)",
        "auth_method": "Box JWT authentication"
      },
      {
        "step": 3,
        "name": "List Tables",
        "action": "Query BigQuery for tables in target dataset",
        "output": "List of table names to export"
      },
      {
        "step": 4,
        "name": "Export to GCS",
        "action": "bq extract to gs://bqx-ml-exports/{dataset}/{table}/*.parquet",
        "format": "Parquet with Snappy compression"
      },
      {
        "step": 5,
        "name": "Download from GCS",
        "action": "gsutil cp to local temp directory",
        "cleanup": "Auto-delete after upload"
      },
      {
        "step": 6,
        "name": "Upload to Box",
        "action": "Upload Parquet files to Box.com folder",
        "destination": "bqx-ml-v3/GCP/bigquery/{dataset}/{table}/",
        "versioning": "Updates existing files, creates new versions"
      },
      {
        "step": 7,
        "name": "Cleanup GCS",
        "action": "gsutil rm temp export files from GCS",
        "purpose": "Minimize storage costs"
      }
    ],
    "scheduled_jobs": [
      {
        "name": "bq-to-box-models",
        "schedule": "0 2 * * *",
        "description": "Daily at 2 AM UTC",
        "dataset": "bqx_ml_v3_models",
        "priority": "CRITICAL - trained models"
      },
      {
        "name": "bq-to-box-features",
        "schedule": "0 3 * * 0",
        "description": "Weekly Sunday 3 AM UTC",
        "dataset": "bqx_ml_v3_features",
        "priority": "HIGH - engineered features"
      },
      {
        "name": "bq-to-box-predictions",
        "schedule": "0 4 * * 1",
        "description": "Weekly Monday 4 AM UTC",
        "dataset": "bqx_ml_v3_predictions",
        "priority": "MEDIUM - model predictions"
      },
      {
        "name": "bq-to-box-analytics",
        "schedule": "0 5 1 * *",
        "description": "Monthly 1st at 5 AM UTC",
        "dataset": "bqx_ml_v3_analytics",
        "priority": "LOW - analytics metrics"
      }
    ],
    "box_folder_structure": {
      "root": "bqx-ml-v3 (353414610676)",
      "bigquery_parent": "GCP/bigquery/ (353417391696)",
      "dataset_folders": {
        "bqx_bq_uscen1": "353419110877",
        "bqx_ml_v3_features": "353419806934",
        "bqx_ml_v3_models": "353417980012",
        "bqx_ml_v3_predictions": "353419935781",
        "bqx_ml_v3_analytics": "353419303017",
        "bqx_ml_v3_staging": "353417344230"
      }
    },
    "cost_estimate": {
      "cloud_run_compute": "$0-2.84/month (free tier covers most usage)",
      "cloud_scheduler": "$0.40/month (4 jobs)",
      "total_monthly": "$0.40-3.24"
    },
    "producer_consumer_model": {
      "producer": "BA Agent creates BigQuery tables (async, continuous)",
      "buffer": "BigQuery datasets (storage)",
      "consumer": "Cloud Run exports to Box (scheduled, independent)",
      "decoupling_benefits": [
        "BA continues without waiting for exports",
        "Exports run on schedule without blocking",
        "Independent scaling of both operations"
      ]
    },
    "documentation": "/docs/CLOUD_RUN_BOX_BACKUP.md",
    "source_files": [
      "containers/bq-to-box/sync_service.py",
      "containers/bq-to-box/Dockerfile",
      "containers/bq-to-box/requirements.txt"
    ],
    "monitoring": ["export_success_rate", "export_duration", "box_api_errors", "scheduler_failures"]
  },
  "monitoring_workflow": {
    "name": "Continuous Monitoring & Alerting",
    "frequency": "Real-time",
    "dashboards": {
      "infrastructure": ["uptime", "latency", "error_rate", "cpu", "memory"],
      "data": ["ingestion_rate", "data_quality", "table_sizes"],
      "models": ["prediction_accuracy", "drift_score", "confidence"],
      "business": ["request_volume", "active_users", "revenue_impact"]
    },
    "alerts": [
      {
        "name": "High Error Rate",
        "condition": "error_rate > 1% for 5 minutes",
        "severity": "critical",
        "action": "Page on-call engineer"
      },
      {
        "name": "Model Drift Detected",
        "condition": "drift_score > 0.3",
        "severity": "warning",
        "action": "Notify ML team, trigger retraining"
      },
      {
        "name": "Data Pipeline Failure",
        "condition": "no new data for 10 minutes",
        "severity": "critical",
        "action": "Alert data team, check data sources"
      },
      {
        "name": "High Latency",
        "condition": "p95_latency > 500ms for 5 minutes",
        "severity": "warning",
        "action": "Alert infrastructure team"
      }
    ]
  }
}
