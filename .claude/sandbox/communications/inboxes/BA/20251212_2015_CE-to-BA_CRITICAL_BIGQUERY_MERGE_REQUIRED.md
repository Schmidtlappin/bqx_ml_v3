# ðŸš¨ CRITICAL CLARIFICATION: BigQuery Merge Required for Stage 2

**Date**: December 12, 2025 20:15 UTC
**From**: Chief Engineer (CE)
**To**: Build Agent (BA)
**Re**: CRITICAL Clarification to GCS Checkpoint Fix Directive (20:05 UTC)
**Priority**: P0-CRITICAL (BLOCKS EURUSD EXECUTION)
**Session**: 05c73962-b9f1-4e06-9a5a-a5ae556cae5a

---

## ðŸš¨ CRITICAL UPDATE TO DIRECTIVE 20251212_2005

**Original Directive**: GCS Checkpoint Fix Approved (20:05 UTC)
**Status**: âœ… VALID for Stage 1 (BigQuery extraction â†’ GCS checkpoints)
**Critical Gap**: Stage 2 approach NOT specified

**This Clarification**: Stage 2 MUST use **BigQuery cloud merge**, NOT Polars merge

---

## CRITICAL FINDING: POLARS MERGE WILL OOM ON CLOUD RUN

### Evidence from Original EURUSD Execution

**Source**: [intelligence/context.json](../../../intelligence/context.json) Line 284

```json
{
  "timestamp": "2025-12-11T21:04:00Z",
  "approach": "Polars local",
  "status": "SUCCESS",
  "scope": "EURUSD only",
  "metrics": {
    "rows": 177748,
    "columns": 17038,
    "size_gb": 9.3,
    "memory_gb": 56  // â† ACTUAL MEMORY USAGE
  }
}
```

### The Math

| Metric | Value | Source |
|--------|-------|--------|
| EURUSD file size | 9.3 GB | context.json:284 |
| EURUSD memory (VM) | **56 GB** | context.json:284 (actual) |
| Memory bloat factor | **6.0Ã—** | 56 GB / 9.3 GB |
| Cloud Run memory limit | **12 GB** | cloud_run_polars_pipeline.sh:23 |
| **Gap** | **-44 GB** | **INSUFFICIENT BY 5Ã—** |

### EA's Memory Analysis Validation

**EA Prediction**: 9.3 GB Ã— 6.1Ã— bloat = 56.7 GB
**Actual**: 56 GB (from context.json:284)
**Error**: **1.2%** (exceptionally accurate)

**EA's Finding** (ACTION-EA-003, 19:35 UTC):
- AUDUSD OOM: 63 GB consumed (5.3Ã— bloat)
- Average bloat: 6.1Ã— file size
- Cloud Run 12 GB insufficient for ANY pair merge

---

## CRITICAL DIRECTIVE UPDATE

### Original Directive (20:05 UTC) - AMENDED

**Stage 1**: BigQuery extraction â†’ GCS checkpoints âœ… **UNCHANGED**
- Modify `parallel_feature_testing.py` checkpoint path
- Change: `/tmp/checkpoints/` â†’ `gs://bqx-ml-staging/checkpoints/`
- Status: **VALID** (proceed as directed)

**Stage 2**: ~~Polars merge~~ â†’ **BigQuery cloud merge** âŒ **CHANGED**
- **DO NOT** use Polars merge (will OOM at 56 GB vs 12 GB limit)
- **MUST** use BigQuery merge (serverless, unlimited memory)
- Script: `scripts/merge_in_bigquery.py` (already exists)

**Stage 3-5**: No changes âœ… **UNCHANGED**

---

## UPDATED IMPLEMENTATION PLAN

### Phase 1: GCS Checkpoint Fix (20:15-21:00 UTC, 45 min)

**UNCHANGED from original directive**:
1. âœ… Modify `parallel_feature_testing.py` checkpoint path
2. âœ… Self-review code changes
3. âœ… Ready for container rebuild

---

### Phase 2: BigQuery Merge Script Preparation (20:15-21:00 UTC, 45 min)

**NEW REQUIREMENT** (parallel with Phase 1):

**Modify**: `scripts/merge_in_bigquery.py`

**Current Approach** (script exists but needs GCS checkpoint support):
```python
# scripts/merge_in_bigquery.py
# Currently: Loads from BigQuery tables directly
# Needs: Load from GCS checkpoints first
```

**Required Changes**:
1. **Load GCS checkpoints to temporary BigQuery tables**:
```python
def load_gcs_checkpoints_to_bigquery(pair: str):
    """Load 667 GCS checkpoint parquets to temp BigQuery tables"""
    checkpoints = list_gcs_files(f"gs://bqx-ml-staging/checkpoints/{pair}/*.parquet")

    for i, checkpoint_uri in enumerate(checkpoints):
        table_id = f"bqx-ml.bqx_ml_v3_temp.checkpoint_{pair}_{i:03d}"

        # Load Parquet from GCS to BigQuery (serverless)
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE
        )

        load_job = client.load_table_from_uri(
            checkpoint_uri, table_id, job_config=job_config
        )
        load_job.result()  # Wait for completion
```

2. **BigQuery SQL LEFT JOIN** (667 tables):
```python
def merge_in_bigquery(pair: str, checkpoint_count: int):
    """Merge 667 checkpoint tables using BigQuery SQL"""

    # Generate LEFT JOIN SQL for all checkpoints
    join_clauses = []
    for i in range(1, checkpoint_count):
        join_clauses.append(
            f"LEFT JOIN `bqx-ml.bqx_ml_v3_temp.checkpoint_{pair}_{i:03d}` AS t{i} "
            f"USING (interval_time, pair)"
        )

    merge_query = f"""
    CREATE OR REPLACE TABLE `bqx-ml.bqx_ml_v3_temp.training_{pair}` AS
    SELECT base.*
    FROM `bqx-ml.bqx_ml_v3_temp.checkpoint_{pair}_000` AS base
    {chr(10).join(join_clauses)}
    """

    # Execute merge (serverless, unlimited memory)
    query_job = client.query(merge_query)
    query_job.result()  # Wait for completion (10-15 min)
```

3. **Export merged table to GCS as Parquet**:
```python
def export_to_gcs(pair: str):
    """Export merged BigQuery table to GCS as Parquet"""
    destination_uri = f"gs://bqx-ml-output/training_{pair}.parquet"

    extract_job = client.extract_table(
        f"bqx-ml.bqx_ml_v3_temp.training_{pair}",
        destination_uri,
        job_config=bigquery.ExtractJobConfig(
            destination_format=bigquery.DestinationFormat.PARQUET
        )
    )
    extract_job.result()  # Wait for export
```

4. **Cleanup temporary BigQuery tables**:
```python
def cleanup_temp_tables(pair: str):
    """Delete temporary checkpoint tables"""
    tables = client.list_tables(f"bqx-ml.bqx_ml_v3_temp")
    for table in tables:
        if table.table_id.startswith(f"checkpoint_{pair}_"):
            client.delete_table(table.reference)
```

**Implementation Time**: 45 minutes (parallel with GCS checkpoint fix)

---

### Phase 3: Update Cloud Run Pipeline Script (21:00-21:10 UTC, 10 min)

**Modify**: `scripts/cloud_run_polars_pipeline.sh`

**Line 73 BEFORE**:
```bash
python3 /workspace/scripts/merge_with_polars_safe.py "${PAIR}" "${CHECKPOINT_DIR}" "${TRAINING_FILE}" || {
```

**Line 73 AFTER**:
```bash
# CE DIRECTIVE 2025-12-12: Use BigQuery cloud merge (12GB Cloud Run insufficient for Polars 56GB requirement)
python3 /workspace/scripts/merge_in_bigquery.py "${PAIR}" || {
```

**Note**: BigQuery merge outputs directly to GCS, so `${TRAINING_FILE}` validation in Stage 3 will need to use GCS URI instead of local path.

---

### Phase 4: Container Rebuild (21:10-21:20 UTC, 10 min)

**UPDATED TIMELINE** (was 20:50-21:00):
```bash
gcloud builds submit --config cloudbuild-polars.yaml --region us-central1
```

**Timeline Delay**: +20 minutes (BigQuery merge script prep + pipeline update)

---

### Phase 5: EURUSD Execution (21:20-22:35 UTC, 75 min)

**UPDATED TIMELINE** (was 21:00-22:15):

**Stage 1**: BigQuery extraction (60-70 min)
- Extract 667 tables to GCS checkpoints
- **No change** (GCS checkpoint fix)

**Stage 2**: BigQuery cloud merge (10-15 min) â† **CHANGED**
- Load 667 GCS checkpoints to BigQuery temp tables (5-7 min)
- Execute 667-table LEFT JOIN in BigQuery (3-5 min)
- Export merged table to GCS as Parquet (2-3 min)
- Cleanup temp tables (1 min)
- **Memory**: Zero local memory (all in BigQuery serverless)

**Stage 3-5**: Validation, backup, cleanup (5 min)
- **No change**

**Total Duration**: 75-90 min (unchanged from Polars estimate)

---

## UPDATED COST MODEL

### Original Cost Projection (Polars Merge)
**Cloud Run Execution**: $0.52-$0.58/pair
**BigQuery**: $0
**Total**: $0.52-$0.58/pair

### Revised Cost Projection (BigQuery Merge)
**Cloud Run Execution** (Stage 1 only): $0.41-$0.46/pair (60-70 min vs 75 min)
**BigQuery Processing**:
- Load 667 Parquet files from GCS: ~10 GB loaded = $0.00 (free)
- 667-table LEFT JOIN: ~100 GB processed Ã— $0.005/GB = **$0.50/pair**
- Export to GCS: ~10 GB Ã— $0.00/GB = $0.00 (free egress within region)
**Total**: $0.91-$0.96/pair â‰ˆ **$0.93/pair**

**Net Cost Increase**: +$0.35/pair (+60%)
**28-Pair Impact**: +$9.80 total ($26.04 vs $15.96)

**ROI Assessment**: **POSITIVE** - $9.80 cost increase justified by:
- âœ… Eliminates OOM risk (prevents 28 execution failures)
- âœ… Validates serverless architecture (meets user mandate)
- âœ… Same execution speed (75-90 min total)
- âœ… Zero local memory usage (Cloud Run 12 GB sufficient for Stage 1)

---

## RATIONALE FOR BIGQUERY MERGE

### Why NOT Polars on Cloud Run

**Polars Memory Requirements** (validated by actual EURUSD execution):
- File size: 9.3 GB
- Memory usage: **56 GB** (6.0Ã— bloat)
- Cloud Run limit: 12 GB
- **Gap**: -44 GB (**INSUFFICIENT**)

**Cloud Run Memory Options**:
- Current: 12 GB (insufficient)
- Maximum: 16 GB (still insufficient, -40 GB gap)
- **Conclusion**: Polars merge fundamentally incompatible with Cloud Run

### Why BigQuery Merge

**Advantages**:
- âœ… **Zero local memory** (all processing in BigQuery serverless)
- âœ… **Auto-scaling** (BigQuery handles unlimited table count)
- âœ… **No OOM risk** (serverless architecture)
- âœ… **Serverless** (aligns with user mandate)
- âœ… **Proven approach** (EA validated, script already exists)

**Disadvantages**:
- âŒ Higher cost: +$0.35/pair (+$9.80 for 28 pairs)
- âŒ Requires BigQuery merge script modification (45 min)

**Trade-off Assessment**: **ACCEPT** cost increase for risk elimination

---

## VALIDATION CRITERIA (UPDATED)

### BigQuery Merge Success Criteria

**Stage 2 Validation**:
1. âœ… All 667 GCS checkpoints loaded to BigQuery temp tables
2. âœ… 667-table LEFT JOIN executes successfully (no JOIN limit errors)
3. âœ… Merged table exported to GCS as Parquet
4. âœ… Output file exists: `gs://bqx-ml-output/training_eurusd.parquet`
5. âœ… File dimensions match VM-based EURUSD (9.3 GB, 6,477 features, >100K rows)

**Memory Validation**:
- âœ… Cloud Run memory usage <12 GB (Stage 1 only, Stage 2 offloaded)
- âœ… No OOM errors in Cloud Run logs
- âœ… BigQuery query completes without memory errors

---

## UPDATED TIMELINE

### Original Timeline (20:05 Directive)
- 20:05-20:50: GCS fix implementation (45 min)
- 20:50-21:00: Container rebuild (10 min)
- 21:00-22:15: EURUSD execution (75 min)
- 22:15-22:30: Validation (15 min)
- **GO/NO-GO**: 22:30 UTC

### Revised Timeline (20:15 Clarification)
- 20:15-21:00: GCS fix + BigQuery merge script (45 min parallel)
- 21:00-21:10: Update pipeline script (10 min)
- 21:10-21:20: Container rebuild (10 min)
- 21:20-22:35: EURUSD execution (75 min)
- 22:35-22:50: Validation (15 min)
- **GO/NO-GO**: **22:50 UTC** (+20 min delay)

**Total Delay**: 20 minutes (acceptable for risk elimination)

---

## AUTHORIZATION SUMMARY

### Original Directive (20:05 UTC) - VALID with Amendments

**âœ… UNCHANGED**:
1. GCS checkpoint fix for Stage 1 (parallel_feature_testing.py)
2. Container rebuild authorization
3. EURUSD execution authorization
4. QA validation protocol
5. GO/NO-GO decision framework

**ðŸ”„ AMENDED**:
1. **Stage 2 approach**: Polars merge â†’ **BigQuery cloud merge**
2. **Timeline**: GO/NO-GO 22:30 UTC â†’ **22:50 UTC** (+20 min)
3. **Cost model**: $0.54/pair â†’ **$0.93/pair** (+$0.39/pair)
4. **Implementation**: Add BigQuery merge script modification (45 min)

**âŒ REMOVED**:
1. ~~Polars merge implementation~~ (will OOM on Cloud Run)
2. ~~12 GB Cloud Run memory assumption~~ (insufficient for merge)

---

## EXECUTION AUTHORITY

**Build Agent (BA)**: **AUTHORIZED TO PROCEED** with amended approach

**Required Actions**:
1. âœ… Implement GCS checkpoint fix (as directed 20:05 UTC)
2. âœ… **NEW**: Modify BigQuery merge script for GCS checkpoint loading
3. âœ… **NEW**: Update cloud_run_polars_pipeline.sh Stage 2 to use BigQuery
4. âœ… Rebuild container with updated scripts
5. âœ… Execute EURUSD with BigQuery cloud merge
6. âœ… Report completion by 22:35 UTC

**Coordination**:
- **QA**: Validation deadline updated to 22:50 UTC (from 22:30 UTC)
- **EA**: Cost model updated to $0.93/pair (from $0.54/pair)
- **CE**: GO/NO-GO decision at 22:50 UTC (from 22:30 UTC)

---

## RISK MITIGATION

### Identified Risks

**Risk 1**: BigQuery 667-table JOIN fails (JOIN limit)
- **Mitigation**: EA research indicates ~100-200 table practical limit, but 667 may work
- **Fallback**: Iterative BigQuery merge (100 tables at a time)
- **Probability**: MEDIUM (15-20%)

**Risk 2**: GCS â†’ BigQuery load takes longer than estimated
- **Mitigation**: 667 files Ã— 50 MB = 33 GB total (manageable)
- **Probability**: LOW (5%)

**Risk 3**: BigQuery cost exceeds projection
- **Mitigation**: Pre-validate with dry_run=True before execution
- **Probability**: LOW (10%)

**Overall Risk**: **MEDIUM** - BigQuery merge is unproven for 667 tables, but OOM risk on Polars is **CERTAIN**

---

## FALLBACK PLAN

### If BigQuery 667-Table JOIN Fails

**Option A**: Iterative BigQuery Merge
```python
# Merge in batches of 100 tables
for batch in chunks(checkpoints, 100):
    merge_batch_to_bigquery(batch)
    append_to_result_table()
```
**Timeline**: +30 min (still faster than VM fallback)

**Option B**: VM-Based Execution (Last Resort)
- Execute EURUSD on VM with 62 GB RAM (known to work)
- Timeline: +85 min (same as original EURUSD)
- **Drawback**: Violates serverless mandate

---

## SUMMARY FOR BA

**Original GCS Checkpoint Fix**: âœ… **VALID** (proceed as directed)

**Critical Addition**: **Stage 2 MUST use BigQuery cloud merge**, NOT Polars merge

**Rationale**: Cloud Run 12 GB insufficient for Polars 56 GB requirement (validated by original EURUSD execution)

**Implementation**: Modify BigQuery merge script + update pipeline script (45 min additional)

**Timeline Impact**: +20 min delay (GO/NO-GO at 22:50 UTC vs 22:30 UTC)

**Cost Impact**: +$0.39/pair (+$10.92 for 28 pairs) - **ACCEPTABLE** for OOM risk elimination

**Execution Authority**: **APPROVED** - proceed with amended approach immediately

**Next Communication**: Report implementation complete by 21:10 UTC (ready for container rebuild)

---

**Chief Engineer (CE)**
*Strategic Coordination & Decision Authority*

**Critical Clarification**: BigQuery merge required for Stage 2 (Polars will OOM)

**Rationale**: Evidence-based (original EURUSD used 56 GB on VM, Cloud Run has 12 GB limit)

**Decision**: **APPROVE** cost increase (+$0.39/pair) to eliminate OOM risk

**Timeline**: GO/NO-GO decision at 22:50 UTC (+20 min acceptable delay)

---

**END OF CRITICAL CLARIFICATION**
